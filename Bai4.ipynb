{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeviathanGod/FuzzyExtractor-1/blob/master/Bai4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J7UTD_Vj8cx"
      },
      "outputs": [],
      "source": [
        "!pip install python-whois"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\n",
        "!unzip -o top-1m.csv.zip"
      ],
      "metadata": {
        "id": "CXaVK_iE1eja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1tcq4RUblt-"
      },
      "outputs": [],
      "source": [
        "!pip install python-whois\n",
        "import pandas as pd\n",
        "data0 = pd.read_csv(\"verified_online.csv\")\n",
        "data0.head()\n",
        "data0.shape\n",
        "#Collecting 5,000 Phishing URLs randomly\n",
        "phishurl = data0.sample(n = 5000, random_state = 12).copy()\n",
        "phishurl = phishurl.reset_index(drop=True)\n",
        "phishurl.head()\n",
        "\n",
        "phishurl.shape\n",
        "\n",
        "#Loading legitimate files \n",
        "data1 = pd.read_csv(\"Benign_list_big_final.csv\")\n",
        "data1.columns = ['URLs']\n",
        "data1.head()\n",
        "\n",
        "#Collecting 5,000 Legitimate URLs randomly\n",
        "legiurl = data1.sample(n = 5000, random_state = 12).copy()\n",
        "legiurl = legiurl.reset_index(drop=True)\n",
        "legiurl.head()\n",
        "\n",
        "legiurl.shape\n",
        "\n",
        "# importing required packages for this section\n",
        "from urllib.parse import urlparse,urlencode\n",
        "import ipaddress\n",
        "import re\n",
        "# 1.Domain of the URL (Domain) \n",
        "def getDomain(url):  \n",
        "  domain = urlparse(url).netloc\n",
        "  if re.match(r\"^www.\",domain):\n",
        "\t       domain = domain.replace(\"www.\",\"\")\n",
        "  return domain\n",
        "\n",
        "  # 2.Checks for IP address in URL (Have_IP)\n",
        "def havingIP(url):\n",
        "  try:\n",
        "    ipaddress.ip_address(url)\n",
        "    ip = 1\n",
        "  except:\n",
        "    ip = 0\n",
        "  return ip\n",
        "\n",
        "  # 3.Checks the presence of @ in URL (Have_At)\n",
        "def haveAtSign(url):\n",
        "  if \"@\" in url:\n",
        "    at = 1    \n",
        "  else:\n",
        "    at = 0    \n",
        "  return at\n",
        "# 4.Finding the length of URL and categorizing (URL_Length)\n",
        "def getLength(url):\n",
        "  if len(url) < 54:\n",
        "    length = 0            \n",
        "  else:\n",
        "    length = 1            \n",
        "  return length\n",
        "\n",
        "  # 5.Gives number of '/' in URL (URL_Depth)\n",
        "def getDepth(url):\n",
        "  s = urlparse(url).path.split('/')\n",
        "  depth = 0\n",
        "  for j in range(len(s)):\n",
        "    if len(s[j]) != 0:\n",
        "      depth = depth+1\n",
        "  return depth\n",
        "\n",
        "  # 6.Checking for redirection '//' in the url (Redirection)\n",
        "def redirection(url):\n",
        "  pos = url.rfind('//')\n",
        "  if pos > 6:\n",
        "    if pos > 7:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "  # 7.Existence of “HTTPS” Token in the Domain Part of the URL (https_Domain)\n",
        "def httpDomain(url):\n",
        "  domain = urlparse(url).netloc\n",
        "  if 'https' in domain:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "#listing shortening services\n",
        "shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
        "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
        "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
        "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
        "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
        "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
        "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
        "                      r\"tr\\.im|link\\.zip\\.net\"\n",
        "\n",
        "# 8. Checking for Shortening Services in URL (Tiny_URL)\n",
        "def tinyURL(url):\n",
        "    match=re.search(shortening_services,url)\n",
        "    if match:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# 9.Checking for Prefix or Suffix Separated by (-) in the Domain (Prefix/Suffix)\n",
        "def prefixSuffix(url):\n",
        "    if '-' in urlparse(url).netloc:\n",
        "        return 1            # phishing\n",
        "    else:\n",
        "        return 0            # legitimate\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import whois\n",
        "import urllib\n",
        "import urllib.request\n",
        "from datetime import datetime\n",
        "\n",
        "# 12.Web traffic (Web_Traffic)\n",
        "import csv\n",
        "\n",
        "with open('top-1m.csv') as f:\n",
        "    reader = csv.reader(f)\n",
        "    alexa = list(reader)\n",
        "\n",
        "def web_traffic(url, alexa):\n",
        "  domain = getDomain(url)\n",
        "  try:\n",
        "    rank = [i for i, v in enumerate(alexa) if v[1] == domain][0] + 1\n",
        "  except:\n",
        "    return 0\n",
        "  if rank <100000:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# 13.Survival time of domain: The difference between termination time and creation time (Domain_Age)  \n",
        "def domainAge(domain_name):\n",
        "  creation_date = domain_name.creation_date\n",
        "  expiration_date = domain_name.expiration_date\n",
        "  if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
        "    try:\n",
        "      creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
        "      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
        "    except:\n",
        "      return 1\n",
        "  if ((expiration_date is None) or (creation_date is None)):\n",
        "      return 1\n",
        "  elif ((type(expiration_date) is list) or (type(creation_date) is list)):\n",
        "      return 1\n",
        "  else:\n",
        "    ageofdomain = abs((expiration_date - creation_date).days)\n",
        "    if ((ageofdomain/30) < 6):\n",
        "      age = 1\n",
        "    else:\n",
        "      age = 0\n",
        "  return age\n",
        "\n",
        "  # 14.End time of domain: The difference between termination time and current time (Domain_End) \n",
        "def domainEnd(domain_name):\n",
        "  expiration_date = domain_name.expiration_date\n",
        "  if isinstance(expiration_date,str):\n",
        "    try:\n",
        "      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
        "    except:\n",
        "      return 1\n",
        "  if (expiration_date is None):\n",
        "      return 1\n",
        "  elif (type(expiration_date) is list):\n",
        "      return 1\n",
        "  else:\n",
        "    today = datetime.now()\n",
        "    end = abs((expiration_date - today).days)\n",
        "    if ((end/30) < 6):\n",
        "      end = 0\n",
        "    else:\n",
        "      end = 1\n",
        "  return end\n",
        "\n",
        "  import requests\n",
        "  # 15. IFrame Redirection (iFrame)\n",
        "def iframe(response):\n",
        "  if response == \"\":\n",
        "      return 1\n",
        "  else:\n",
        "      if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
        "          return 0\n",
        "      else:\n",
        "          return 1\n",
        "\n",
        "#Function to extract features\n",
        "def featureExtraction(url,label):\n",
        "\n",
        "  features = []\n",
        "  #Address bar based features (10)\n",
        "  features.append(getDomain(url))\n",
        "  features.append(havingIP(url))\n",
        "  features.append(haveAtSign(url))\n",
        "  features.append(getLength(url))\n",
        "  features.append(getDepth(url))\n",
        "  features.append(redirection(url))\n",
        "  features.append(httpDomain(url))\n",
        "  features.append(tinyURL(url))\n",
        "  features.append(prefixSuffix(url))\n",
        "  \n",
        "  #Domain based features (4)\n",
        "  dns = 0\n",
        "  try:\n",
        "    domain_name = whois.whois(urlparse(url).netloc)\n",
        "  except:\n",
        "    dns = 1\n",
        "\n",
        "  features.append(dns)\n",
        "  features.append(web_traffic(url,alexa))\n",
        "  features.append(1 if dns == 1 else domainAge(domain_name))\n",
        "  features.append(1 if dns == 1 else domainEnd(domain_name))\n",
        "\n",
        "  return features   \n",
        "\n",
        "legi_features = []\n",
        "label = 0\n",
        "\n",
        "for i in range(0, 5000):\n",
        "  url = legiurl['URLs'][i]\n",
        "  legi_features.append(featureExtraction(url,label))\n",
        "\n",
        "feature_names = ['Domain', 'Have_IP', 'Have_At', 'URL_Length', 'URL_Depth','Redirection', \n",
        "                      'https_Domain', 'TinyURL', 'Prefix/Suffix', 'DNS_Record', 'Web_Traffic', \n",
        "                      'Domain_Age', 'Domain_End', 'iFrame', 'Label']\n",
        "\n",
        "legitimate = pd.DataFrame(legi_features, columns= feature_names)\n",
        "legitimate.head()\n",
        "\n",
        "legitimate.to_csv('legitimate.csv', index= False)\n",
        "\n",
        "#Extracting the feautres & storing them in a list\n",
        "phish_features = []\n",
        "label = 1\n",
        "for i in range(0, 5000):\n",
        "  url = phishurl['url'][i]\n",
        "  phish_features.append(featureExtraction(url,label))\n",
        "\n",
        "#converting the list to dataframe\n",
        "feature_names = ['Domain', 'Have_IP', 'Have_At', 'URL_Length', 'URL_Depth','Redirection', \n",
        "                      'https_Domain', 'TinyURL', 'Prefix/Suffix', 'DNS_Record', 'Web_Traffic', \n",
        "                      'Domain_Age', 'Domain_End', 'iFrame', 'Label']\n",
        "\n",
        "phishing = pd.DataFrame(phish_features, columns= feature_names)\n",
        "phishing.head()\n",
        "\n",
        "phishing.to_csv('phishing.csv', index= False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOj29uzsQ5DGes8d+/SWtjt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}